"""
æ ‡å‡†åŒ–æ•°æ®ç¼“å­˜æœåŠ¡

ç»Ÿä¸€çš„ç¼“å­˜æ¶æ„ï¼Œæ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼š
- Kçº¿æ•°æ® (Candlestick)
- èµ„é‡‘è´¹ç‡ (Funding Rate)
- åˆçº¦åŸºå·® (Contract Basis)
- Tickeræ•°æ®

ä½¿ç”¨Redisä½œä¸ºä¸»è¦ç¼“å­˜å±‚ï¼Œæ”¯æŒï¼š
- å•å€¼ç¼“å­˜ï¼ˆæ™®é€škey-valueï¼‰
- æ—¶é—´åºåˆ—ç¼“å­˜ï¼ˆsorted setï¼‰
- è‡ªåŠ¨è¿‡æœŸï¼ˆTTLï¼‰
- æ‰¹é‡æ“ä½œ
"""
import json
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Union
from enum import Enum

from django.conf import settings

from .redis_cache import get_redis_client, redis_cache_enabled

try:
    from redis.exceptions import RedisError
except Exception:
    class RedisError(Exception):
        pass

logger = logging.getLogger(__name__)


class CacheDataType(Enum):
    """ç¼“å­˜æ•°æ®ç±»å‹"""
    CANDLESTICK = "candles"
    FUNDING_RATE = "funding_rate"
    FUNDING_HISTORY = "funding_history"
    CONTRACT_BASIS = "basis"
    BASIS_HISTORY = "basis_history"
    TICKER = "ticker"


@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    ttl: int  # è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
    max_entries: Optional[int] = None  # æ—¶é—´åºåˆ—æœ€å¤§æ¡ç›®æ•°
    key_prefix: str = ""  # keyå‰ç¼€
    
    @classmethod
    def for_type(cls, data_type: CacheDataType) -> 'CacheConfig':
        """æ ¹æ®æ•°æ®ç±»å‹è¿”å›é»˜è®¤é…ç½®"""
        configs = {
            CacheDataType.CANDLESTICK: cls(ttl=86400, max_entries=5000, key_prefix="candles"),
            CacheDataType.FUNDING_RATE: cls(ttl=3600, key_prefix="funding_rate"),
            CacheDataType.FUNDING_HISTORY: cls(ttl=86400, max_entries=200, key_prefix="funding_history"),
            CacheDataType.CONTRACT_BASIS: cls(ttl=1800, key_prefix="basis"),
            CacheDataType.BASIS_HISTORY: cls(ttl=3600, max_entries=1000, key_prefix="basis_history"),
            CacheDataType.TICKER: cls(ttl=30, key_prefix="ticker"),
        }
        return configs.get(data_type, cls(ttl=3600))


class BaseCacheService(ABC):
    """ç¼“å­˜æœåŠ¡åŸºç±»"""
    
    def __init__(self, data_type: CacheDataType, config: Optional[CacheConfig] = None):
        self.data_type = data_type
        self.config = config or CacheConfig.for_type(data_type)
        
    @staticmethod
    def _redis_client():
        """è·å–Rediså®¢æˆ·ç«¯"""
        if not redis_cache_enabled():
            return None
        return get_redis_client()
    
    @abstractmethod
    def build_key(self, **params) -> str:
        """æ„å»ºç¼“å­˜key"""
        pass
    
    def get(self, **params) -> Optional[Any]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        client = self._redis_client()
        if not client:
            return None
        
        key = self.build_key(**params)
        try:
            data = client.get(key)
            if data:
                logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {key}")
                return json.loads(data)
        except (RedisError, json.JSONDecodeError) as e:
            logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥ {key}: {e}")
        return None
    
    def set(self, data: Any, **params) -> bool:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        client = self._redis_client()
        if not client:
            return False
        
        key = self.build_key(**params)
        try:
            serialized = json.dumps(data, separators=(',', ':'))
            client.setex(key, self.config.ttl, serialized)
            logger.debug(f"ğŸ’¾ å·²ç¼“å­˜: {key} (TTL={self.config.ttl}s)")
            return True
        except (RedisError, TypeError) as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥ {key}: {e}")
            return False
    
    def delete(self, **params) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        client = self._redis_client()
        if not client:
            return False
        
        key = self.build_key(**params)
        try:
            client.delete(key)
            logger.debug(f"ğŸ—‘ï¸  å·²åˆ é™¤ç¼“å­˜: {key}")
            return True
        except RedisError as e:
            logger.error(f"åˆ é™¤ç¼“å­˜å¤±è´¥ {key}: {e}")
            return False


class TimeSeriesCacheService(BaseCacheService):
    """æ—¶é—´åºåˆ—ç¼“å­˜æœåŠ¡ï¼ˆä½¿ç”¨sorted setï¼‰"""
    
    def get_series(self, **params) -> Optional[List[Dict[str, Any]]]:
        """è·å–æ—¶é—´åºåˆ—æ•°æ®"""
        client = self._redis_client()
        if not client:
            return None
        
        key = self.build_key(**params)
        try:
            data = client.zrange(key, 0, -1)
            if data:
                logger.debug(f"âœ… æ—¶é—´åºåˆ—ç¼“å­˜å‘½ä¸­: {key}, {len(data)}æ¡")
                return [json.loads(item) for item in data]
        except (RedisError, json.JSONDecodeError) as e:
            logger.warning(f"è¯»å–æ—¶é—´åºåˆ—ç¼“å­˜å¤±è´¥ {key}: {e}")
        return None
    
    def set_series(self, series_data: List[Dict[str, Any]], **params) -> bool:
        """ä¿å­˜æ—¶é—´åºåˆ—æ•°æ®"""
        client = self._redis_client()
        if not client or not series_data:
            return False
        
        key = self.build_key(**params)
        try:
            pipe = client.pipeline(transaction=False)
            
            # æ¸…ç©ºæ—§æ•°æ®
            pipe.delete(key)
            
            # æ·»åŠ æ‰€æœ‰æ•°æ®ç‚¹ï¼Œä½¿ç”¨timestampä½œä¸ºscore
            for item in series_data:
                score = item.get('timestamp') or item.get('time', 0)
                serialized = json.dumps(item, separators=(',', ':'))
                pipe.zadd(key, {serialized: score})
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´
            pipe.expire(key, self.config.ttl)
            pipe.execute()
            
            # ä¿®å‰ªè¿‡é•¿çš„åºåˆ—
            if self.config.max_entries:
                self._trim_series(client, key)
            
            logger.debug(f"ğŸ’¾ å·²ç¼“å­˜æ—¶é—´åºåˆ—: {key}, {len(series_data)}æ¡ (TTL={self.config.ttl}s)")
            return True
        except (RedisError, TypeError) as e:
            logger.error(f"ä¿å­˜æ—¶é—´åºåˆ—ç¼“å­˜å¤±è´¥ {key}: {e}")
            return False
    
    def _trim_series(self, client, key: str) -> None:
        """ä¿®å‰ªè¿‡é•¿çš„æ—¶é—´åºåˆ—ï¼Œä¿ç•™æœ€æ–°çš„Næ¡"""
        try:
            size = client.zcard(key)
            if size > self.config.max_entries:
                # åˆ é™¤æœ€æ—§çš„æ•°æ®
                remove_count = size - self.config.max_entries
                client.zremrangebyrank(key, 0, remove_count - 1)
                logger.debug(f"ğŸ”§ ä¿®å‰ªæ—¶é—´åºåˆ— {key}: åˆ é™¤æœ€æ—§çš„{remove_count}æ¡")
        except RedisError as e:
            logger.warning(f"ä¿®å‰ªæ—¶é—´åºåˆ—å¤±è´¥ {key}: {e}")
    
    def append_to_series(self, item: Dict[str, Any], **params) -> bool:
        """è¿½åŠ å•æ¡æ•°æ®åˆ°æ—¶é—´åºåˆ—"""
        client = self._redis_client()
        if not client:
            return False
        
        key = self.build_key(**params)
        try:
            score = item.get('timestamp') or item.get('time', 0)
            serialized = json.dumps(item, separators=(',', ':'))
            
            pipe = client.pipeline(transaction=False)
            pipe.zadd(key, {serialized: score})
            pipe.expire(key, self.config.ttl)
            pipe.execute()
            
            # ä¿®å‰ª
            if self.config.max_entries:
                self._trim_series(client, key)
            
            logger.debug(f"â• è¿½åŠ åˆ°æ—¶é—´åºåˆ—: {key}")
            return True
        except (RedisError, TypeError) as e:
            logger.error(f"è¿½åŠ æ—¶é—´åºåˆ—å¤±è´¥ {key}: {e}")
            return False


# ===== å…·ä½“å®ç° =====

class CandlestickCache(TimeSeriesCacheService):
    """Kçº¿æ•°æ®ç¼“å­˜"""
    
    def __init__(self):
        super().__init__(CacheDataType.CANDLESTICK)
    
    def build_key(self, source: str, symbol: str, bar: str, mode: str = 'spot', **kwargs) -> str:
        return f"{self.config.key_prefix}:{source.lower()}:{symbol.upper()}:{mode.lower()}:{bar.lower()}"


class FundingRateCache(BaseCacheService):
    """èµ„é‡‘è´¹ç‡å½“å‰å€¼ç¼“å­˜"""
    
    def __init__(self):
        super().__init__(CacheDataType.FUNDING_RATE)
    
    def build_key(self, source: str, symbol: str, **kwargs) -> str:
        return f"{self.config.key_prefix}:{source.lower()}:{symbol.upper()}"


class FundingHistoryCache(TimeSeriesCacheService):
    """èµ„é‡‘è´¹ç‡å†å²ç¼“å­˜"""
    
    def __init__(self):
        super().__init__(CacheDataType.FUNDING_HISTORY)
    
    def build_key(self, source: str, symbol: str, **kwargs) -> str:
        return f"{self.config.key_prefix}:{source.lower()}:{symbol.upper()}"


class ContractBasisCache(BaseCacheService):
    """åˆçº¦åŸºå·®å½“å‰å€¼ç¼“å­˜"""
    
    def __init__(self):
        super().__init__(CacheDataType.CONTRACT_BASIS)
    
    def build_key(self, source: str, symbol: str, contract_type: str = 'perpetual', **kwargs) -> str:
        return f"{self.config.key_prefix}:{source.lower()}:{symbol.upper()}:{contract_type.lower()}"


class BasisHistoryCache(TimeSeriesCacheService):
    """åˆçº¦åŸºå·®å†å²ç¼“å­˜"""
    
    def __init__(self):
        super().__init__(CacheDataType.BASIS_HISTORY)
    
    def build_key(self, source: str, symbol: str, contract_type: str = 'perpetual', **kwargs) -> str:
        return f"{self.config.key_prefix}:{source.lower()}:{symbol.upper()}:{contract_type.lower()}"


class TickerCache(BaseCacheService):
    """Tickeræ•°æ®ç¼“å­˜"""
    
    def __init__(self):
        super().__init__(CacheDataType.TICKER)
    
    def build_key(self, source: str, symbol: str, mode: str = 'spot', **kwargs) -> str:
        return f"{self.config.key_prefix}:{source.lower()}:{symbol.upper()}:{mode.lower()}"


# ===== ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ =====

class UnifiedCacheManager:
    """ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.candlestick = CandlestickCache()
        self.funding_rate = FundingRateCache()
        self.funding_history = FundingHistoryCache()
        self.basis = ContractBasisCache()
        self.basis_history = BasisHistoryCache()
        self.ticker = TickerCache()
    
    def clear_all(self, pattern: Optional[str] = None) -> int:
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜æˆ–åŒ¹é…ç‰¹å®šæ¨¡å¼çš„ç¼“å­˜"""
        client = BaseCacheService._redis_client()
        if not client:
            return 0
        
        try:
            if pattern:
                keys = client.keys(pattern)
            else:
                # æ¸…é™¤æ‰€æœ‰æ•°æ®ç±»å‹çš„ç¼“å­˜
                patterns = [
                    'candles:*',
                    'funding_rate:*',
                    'funding_history:*',
                    'basis:*',
                    'basis_history:*',
                    'ticker:*'
                ]
                keys = []
                for p in patterns:
                    keys.extend(client.keys(p))
            
            if keys:
                deleted = client.delete(*keys)
                logger.info(f"ğŸ—‘ï¸  å·²æ¸…é™¤ {deleted} ä¸ªç¼“å­˜æ¡ç›®")
                return deleted
            return 0
        except RedisError as e:
            logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
            return 0
    
    def get_cache_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        client = BaseCacheService._redis_client()
        if not client:
            return {}
        
        try:
            stats = {}
            for data_type in CacheDataType:
                config = CacheConfig.for_type(data_type)
                pattern = f"{config.key_prefix}:*"
                keys = client.keys(pattern)
                stats[data_type.value] = len(keys)
            return stats
        except RedisError as e:
            logger.error(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {}


# å…¨å±€å•ä¾‹
_cache_manager = None

def get_cache_manager() -> UnifiedCacheManager:
    """è·å–å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _cache_manager
    if _cache_manager is None:
        _cache_manager = UnifiedCacheManager()
    return _cache_manager
